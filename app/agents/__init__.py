from langchain.agents import BaseAgent
from langchain.tools import Tool
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from typing import List

class BaseAgent(BaseAgent):
    def __init__(self, tools: List[Tool] = [], model: ChatOpenAI = None, temperature: float = 0.7):
        """
        Base class for LangChain agents. Can be extended for specific agent functionality.
        
        :param tools: External tools the agent can use (e.g., RAG, APIs).
        :param model: The language model the agent will use for processing.
        :param temperature: Controls randomness in model responses.
        """
        self.tools = tools  # Tools (e.g., RAG, APIs) the agent will use.
        self.model = model or ChatOpenAI(model="gpt-4", temperature=temperature)  # Default model is GPT-4.
        self.llm_chain = None  # To be initialized for specific agents if needed.

    def _run(self, *args, **kwargs):
        """
        Method to be implemented by specific agents. Defines the core functionality (e.g., search, classification).
        
        :return: Result of the agent's operation (e.g., search or classification).
        """
        raise NotImplementedError("The '_run' method must be implemented by a specific agent.")

    def _arun(self, *args, **kwargs):
        """
        Async method to be implemented for agents needing async operations.
        
        :return: Async result of the agent's operation.
        """
        raise NotImplementedError("The '_arun' method must be implemented by a specific agent.")

    def create_llm_chain(self, prompt_template: str) -> LLMChain:
        """
        Helper method to create an LLMChain with a given prompt template.
        
        :param prompt_template: The template for structuring the prompt.
        :return: An LLMChain configured with the model and prompt template.
        """
        prompt = PromptTemplate(input_variables=["input"], template=prompt_template)
        self.llm_chain = LLMChain(llm=self.model, prompt=prompt)
        return self.llm_chain

    def generate_response(self, prompt: str) -> str:
        """
        Generate a response using the model based on the provided prompt.
        
        :param prompt: The input prompt for the model.
        :return: The response generated by the model.
        """
        if self.llm_chain:
            return self.llm_chain.run(input=prompt)  # Use LLMChain if created.
        return self.model.run(prompt)  # Fallback to using the model directly.

    def get_tool_results(self, query: str) -> str:
        """
        Helper method to get results from external tools (e.g., RAG).
        
        :param query: The query to process.
        :return: The results obtained from the tools.
        """
        results = ""
        for tool in self.tools:
            results += tool.run(query)  # Run each tool and append results.
        return results


# Example of a SearchAgent extending BaseAgent
class SearchAgent(BaseAgent):
    def __init__(self, tools: List[Tool]):
        super().__init__(tools=tools)
    
    def _run(self, query: str):
        # Perform a search using external tools (e.g., Elasticsearch, database).
        results = self.get_tool_results(query)
        return results
    
    def _arun(self, query: str):
        # Async search method if needed.
        return self._run(query)

